# Lecture 3 From Word2Vec to Transfomers

## In-class Material

1. [Slides](../slides/w3.pdf)

2. [Notebook](https://github.com/rz0718/BT5153_2025/tree/main/codes/lab_lecture03)

### Extra Reading

1. [RNN vs Autoregressive Models: Transformer](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

2. [Awesome BERT & Transfer Learning in NLP ](https://github.com/cedrickchee/awesome-bert-nlp)

3. [Why BERT Fails In Commercial Environments](https://www.intel.ai/bert-commercial-environments/)

4. [When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

5. [The Time Series Transformer](https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3)

6. [Applying massive language models in the real world with Cohere](http://jalammar.github.io/applying-large-language-models-cohere/)

7. [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html?utm_source=substack&utm_medium=email)