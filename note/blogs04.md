# Lecture 4 LLM and Its Practices I

## In-class Material

1. [Slides](../slides/w4.pdf)

2. [Notebook](https://github.com/rz0718/BT5153_2025/tree/main/codes/lab_lecture04)

### Extra Reading

1. [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)

2. [GPT1: Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

3. [GPT2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

4. [GPT3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

5. [InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

6. [Llama2](https://github.com/meta-llama/llama/tree/main)

7. [Ilya Sutskever: "pretraining is done. we are now in the post training era"](https://www.youtube.com/watch?v=YD-9NG1Ke5Y)

8. [How Scaling Laws Will Determine AI's Future](https://www.youtube.com/watch?v=d6Ed5bZAtrM)